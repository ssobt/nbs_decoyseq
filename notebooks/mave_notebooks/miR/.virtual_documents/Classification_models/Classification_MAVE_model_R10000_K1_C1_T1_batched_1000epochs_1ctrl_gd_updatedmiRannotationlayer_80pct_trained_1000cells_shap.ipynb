import os
os.chdir('/home/ssobti/projects/mir_tud/packages/ciberatac-main/src/mave')
import pandas as pd
import numpy as np
import scanpy as sc
import itertools
import pickle
import shelve


sc.settings.verbosity = 3        # verbosity: errors (0), warnings (1), info (2), hints (3)
sc.logging.print_header()
sc.settings.set_figure_params(dpi=80, facecolor='white')


adata = sc.read('/home/ssobti/projects/mir_tud/010523_filtered_data/miR.concat.raw.adata_gex.guide_assigned.h5ad')


adata2 = sc.read('/home/ssobti/projects/mir_tud/010523_filtered_data/miR.integrated.qc_norm_scaled.dimred.clustered.gex.guide_assigned.h5ad')


adata2.obs


## make sure to only keep that were present in Larisa's normalized data
cells_to_keep = [cell for cell in adata2.obs.index.to_list() if cell in adata.obs.index.to_list()]
adata = adata[cells_to_keep,:]


list(adata.obs.index) == list(adata2.obs.index)


adata.obs.loc[:,'guide'] = adata2.obs.loc[:,'miR.family']


adata.obs


del adata2


### feed in raw matrix (adata.X) into VAE with filter out cells with low # genes and genes expressed in low # of cells
adata.var_names_make_unique()


adata.shape


sc.pl.highest_expr_genes(adata, n_top=20)


sc.pp.filter_cells(adata, min_genes=200)
sc.pp.filter_genes(adata, min_cells=3)


adata.shape


adata.var['mt'] = adata.var_names.str.startswith('MT-')  # annotate the group of mitochondrial genes as 'mt'
sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True)


sc.pl.violin(adata, ['n_genes_by_counts', 'total_counts', 'pct_counts_mt'],
             jitter=0.4, multi_panel=True)


sc.pl.scatter(adata, x='total_counts', y='pct_counts_mt',)
sc.pl.scatter(adata, x='total_counts', y='n_genes_by_counts')


adata = adata[adata.obs.pct_counts_mt < 15, :]


sc.pl.scatter(adata, x='total_counts', y='pct_counts_mt',)
sc.pl.scatter(adata, x='total_counts', y='n_genes_by_counts')


adata.shape


### remove ('hsa-miR-92a-3p', ['miR_1:TCACAAGTCGGCATAT-1']) from adata
cells_to_keep = [cell for cell in adata.obs.index.to_list() if cell != 'miR_1:TCACAAGTCGGCATAT-1']
adata = adata[cells_to_keep,:]


adata.shape


### making all 20 control guides the same annotation
meta = adata.obs
meta = meta.astype({'guide':'string'})
meta.loc[meta.loc[:,'guide'] == 'NA', 'guide'] = 'TuD_NC'
meta = meta.astype({'guide':'category'})
adata.obs = meta
meta.dtypes


adata.obs


### pertubation annotation to add to each cell
one_hot_df = pd.get_dummies(adata.obs["guide"])


one_hot_df


## add the gene annotation for the first layer
## the first layer is connected to the second in a way such that reflects pathways
## RBP_gene_df is the gene connections (first layer) to TFs (second layer) gmt file


import scipy.sparse as sp_sparse
import tables
from itertools import chain
from model import loss_function
from model import VAE
import numpy as np
import os
import pandas as pd
import torch
from gseapy import read_gmt
from datetime import datetime



miR_targets_annot = pd.read_csv('/home/ssobti/projects/mir_tud/uploaded_data/Predicted_Targets_Info.default_predictions.txt', sep='\t')


miR_targets_annot = miR_targets_annot.loc[:, ['miR Family', 'Gene Symbol']]
miR_targets_annot


miR_families = list(set(miR_targets_annot.loc[:,'miR Family']))


gmt_file = {}
for idx, miR_fam in enumerate(miR_families):
    targets = miR_targets_annot.loc[miR_fam == miR_targets_annot.loc[:, 'miR Family'], 'Gene Symbol']
    gmt_file[miR_fam] = list(targets)


### creating a matrix that links gene sets (columns) to master regulators (rows) via 0/1 annotation
gmt_file = pd.Series(gmt_file).str.join('|')
gmt_file = gmt_file.str.get_dummies()
gmt_file


## remove genes not in intersection bw expression matrix and gene_set annotation df
a = set(gmt_file.columns.to_list())
b = set(adata.var.index.to_list())

intersecting_genes = list(a.intersection(b))
print('Genes in annotated gene sets', len(a))
print('Genes in expression matrix', len(b))
print('Union', len(a.union(b)))
print('Intersecting genes kept', len(a.intersection(b)))

adata = adata[:, intersecting_genes]
gmt_file = gmt_file.loc[:, intersecting_genes]


gmtmat_df = gmt_file.transpose()
gmtmat_df


gmtmat = gmtmat_df.to_numpy()
gmtmat


device='cpu'


gmttensor = torch.from_numpy(
            np.transpose(gmtmat)).to(device).long()
gmttensor


## this function tells you how many paramters are in a model
def get_n_params(model):
    pp = 0
    for p in list(model.parameters()):
        nn = 1
        for s in list(p.size()):
            nn = nn * s
        pp += nn
    return pp


import random
samps=random.sample(list(adata.obs.guide.unique()),2)
samps


adata.obs.index


ct_cbc_dict = {}
cbc_ct_dict = {}
for ct in list(adata.obs.guide.unique()):
    ct_cbc_dict[ct] = adata.obs.query(f'guide==\"{ct}\"').index.tolist()
    for cbc in ct_cbc_dict[ct]:
        cbc_ct_dict[cbc] = ct
list(ct_cbc_dict.items())[1]


list(cbc_ct_dict.items())[1]


adata.X.toarray().shape


adata.var.shape


## this converts the expression matrix to a dictionary 
## where each cell/row of matrix is now a 'key' with associated gene expression values

cbc_arr_dict = {}
for cbc, arr in zip(adata.obs.index.tolist(), adata.X.toarray()):
    cbc_arr_dict[cbc] = arr.copy()
list(cbc_arr_dict.items())[1]


## this converts the cell barcode to guide annotation df to a dictionary 
one_hot_dict = one_hot_df.T.to_dict(orient='list')
list(one_hot_dict.keys())[0]


import numpy as np
import random

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torch.nn.functional as F
import torchvision
from torchvision import transforms

class tripletDataset(Dataset):
    def __init__(self, cbc_list, ct_list, ct_cbc_dict, cbc_ct_dict, cbc_arr_dict, one_hot_dict):
        self.cbc_list = cbc_list
        self.ct_list = ct_list
        self.ct_cbc_dict = ct_cbc_dict
        self.cbc_ct_dict = cbc_ct_dict
        self.cbc_arr_dict = cbc_arr_dict
        self.one_hot_dict = one_hot_dict
    
    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        cbc_anchor = self.cbc_list[idx]
        ct_anchor = self.cbc_ct_dict[cbc_anchor]

        #for negative, pick two random cell types, in case the 1st one turned out to be the same as the anchor's cell type
        ct1, ct2 = random.sample(self.ct_list, 2) 
        ct_neg = ct1
        if ct1==ct_anchor:
            ct_neg = ct2
        
        #for positive, pick two cell barcodes, in case the 1st one turned out to be the anchor itself
        cbc1, cbc2 = random.sample(self.ct_cbc_dict[ct_anchor], 2)
        cbc_pos = cbc1
        if cbc1==cbc_anchor:
            cbc_pos = cbc2
        
        cbc_neg = random.choice(self.ct_cbc_dict[ct_neg])
        
        ohot_arr = np.array(self.one_hot_dict[cbc_anchor])
                
        return (torch.from_numpy(cbc_arr_dict[cbc_anchor].astype(np.float32)),
                torch.from_numpy(cbc_arr_dict[cbc_pos].astype(np.float32)),
                torch.from_numpy(cbc_arr_dict[cbc_neg].astype(np.float32)),
                torch.from_numpy(ohot_arr.astype(np.float32)), cbc_anchor, idx)
    
    def __len__(self):
        return len(self.cbc_list)



import random
import math


len(adata.obs.index.tolist())


## only keep cells in data that's fed to DataLoader such that even number of cells across the guides
guide_list = set(list(cbc_ct_dict.values()))


counter_tbl = pd.DataFrame({'guide' : list(cbc_ct_dict.values())})
counter_tbl2 = pd.DataFrame({'guide' : counter_tbl['guide'].value_counts()})


## note TuD_NC has 3000+ cells, x-axis is cut off here
import seaborn as sns
sns.histplot(data=counter_tbl2, x="guide", bins=1000).set_xlim(0,250)


counter_tbl['guide'].value_counts()[-1:-38:-1]


random.seed(5)
new = []
min_cell_cutoff = 30
[new.append(random.sample(ct_cbc_dict[val], min_cell_cutoff)) for idx, val in enumerate(guide_list) if counter_tbl['guide'].value_counts()[val] > (min_cell_cutoff-1)]
randomized_cbc_list = [item for sublist in new for item in sublist]
randomized_cbc_list[0:5]


test_cells = list(set(adata.obs.index.tolist()).difference(randomized_cbc_list))


len(randomized_cbc_list)


def get_key(val, dictionary):
    for key, value in dictionary.items():
        if val in value:
            return key
 
    return "key doesn't exist"


ct_cbc_dict_edited = ct_cbc_dict.copy()


test_cells_set = set(test_cells)
for key, vals in ct_cbc_dict.items():
        ct_cbc_dict_edited[key] = list(set(vals).difference(test_cells_set))
        if len(ct_cbc_dict_edited[key]) == 0:
            del ct_cbc_dict_edited[key]


len(ct_cbc_dict_edited)


get_key(test_cells[0], ct_cbc_dict_edited) ## should not find a key since these cells should be deleted from the dictionary


cbc_ct_dict_edited = cbc_ct_dict.copy()


for key in cbc_ct_dict:
    if key in test_cells:
        del cbc_ct_dict_edited[key]


from itertools import islice
dict(islice(cbc_ct_dict.items(), 0, 2))


len(list(adata.obs.guide.unique()))


len(set(cbc_ct_dict_edited.values()))


cbc_arr_dict_edited = cbc_arr_dict.copy()


for key in cbc_arr_dict:
    if key in test_cells:
        del cbc_arr_dict_edited[key]


one_hot_dict_edited = one_hot_dict.copy()


for key in one_hot_dict:
    if key in test_cells:
        del one_hot_dict_edited[key]


size_of_batch = 1800


triplet_loader = DataLoader(tripletDataset(randomized_cbc_list, list(set(cbc_ct_dict_edited.values())), ct_cbc_dict_edited, cbc_ct_dict_edited, cbc_arr_dict_edited, one_hot_dict_edited), batch_size=size_of_batch, shuffle=True)


comparison_array = adata[randomized_cbc_list,:].X.toarray()


comparison_array.shape


comparison_array


adata.X.toarray().shape[0]


int(comparison_array.shape[0] / triplet_loader.batch_size)


numlvs = 10
vae = VAE(adata.shape[1],  # num genes
              gmttensor, #gmttensor
              len(set(adata[randomized_cbc_list,:].obs["guide"])), #number of cell types
              0,  # batch
              0,  # labels
              gmtmat.shape[1],  # hiddensize
              numlvs)

n_params = get_n_params(vae)
print("VAE has {} parameters".format(n_params))
vae.to(device)


optimizer = torch.optim.Adam(
        vae.parameters(), lr=0.002)

SAMPLE_IDXS = adata.shape[0]


MINIBATCH = size_of_batch
MAXEPOCH = 400
loss_scalers = [10000, 1, 1, 1] #[recon_loss, KLD_loss, classification_loss, triplet_loss]
predict_celltypes = True
num_celltypes = len(set(cbc_ct_dict_edited.values()))
from scipy.stats import pearsonr
arrayed_data = adata[randomized_cbc_list,:].X.toarray()
R_tbl = pd.DataFrame(0, index=range(MAXEPOCH), columns=range(adata[randomized_cbc_list,:].shape[0]))

def train_model(vae, optimizer, data_loader, MAXEPOCH, expar, logdir,
                modelpath, chkpath, loss_scalers, predict_celltypes):
    criterion_class = torch.nn.CrossEntropyLoss()
    time_str = str(datetime.now())
    time_str = time_str.replace(" ", "_")
    time_str = time_str.replace(":", "0")
    logpath = os.path.join(
        logdir,
        "training.log.{}".format(time_str))
    loglink = open(logpath, "w")
    header = ["Epoch", "Training.Loss", "Recon_Loss", "KLD_Loss", "Classification_Loss", "Triplet_Loss", "MiniBatch.ID", "Time.Stamp"]
    loglink.write("\t".join(header) + "\n")
    loglink.close()
    TOTBATCHIDX = int(expar.shape[0] / data_loader.batch_size)
    
    for epoch in range(MAXEPOCH):
        running_loss = 0
        for idxbatch, batch in enumerate(data_loader):
            anchor, pos, neg, ohot, barcds, ids = [x.to(device).float() if ind < 4 else x for ind,x in enumerate(batch)]
            local_l_mean = np.mean(
                np.apply_along_axis(
                    np.sum, 1, anchor.detach().numpy()))
            local_l_var = np.var(
                np.apply_along_axis(
                    np.sum, 1, anchor.detach().numpy()))
            outdict_anchor = vae(anchor)
            outdict_pos = vae(pos)
            outdict_neg = vae(neg)

            ct_pred = outdict_anchor["ctpred"]
            loss_1, loss_2 = loss_function(
                outdict_anchor['qz_m'], outdict_anchor['qz_v'], anchor,
                outdict_anchor['px_rate'], outdict_anchor['px_r'],
                outdict_anchor['px_dropout'], outdict_anchor['ql_m'],
                outdict_anchor['ql_v'], True,
                local_l_mean, local_l_var)
            reconst = outdict_anchor['px_scale'].cpu().detach().numpy()
            Rs = []
            arrayed_data_ordered = arrayed_data[ids.tolist(),:]
            for i in range(len(reconst)):
                Rs.append(pearsonr(arrayed_data_ordered[i,:], reconst[i,:])[0])
            R_tbl.iloc[epoch, ids.tolist()] = Rs
            loss_1 = torch.mean(loss_1)
            loss_2 = torch.mean(loss_2)
            optimizer.zero_grad()
            if predict_celltypes:
                one_hot_temp = torch.max(ohot, 1)[1].to(device).long()
                loss_3 = criterion_class(ct_pred, one_hot_temp)
            else:
                loss_3 = 0

            distance_positive = (outdict_anchor['qz_m'] - outdict_pos['qz_m']).pow(2).sum(1)  # .pow(.5)
            distance_negative = (outdict_anchor['qz_m'] - outdict_neg['qz_m']).pow(2).sum(1)  # .pow(.5)
            losses = F.relu(distance_positive - distance_negative + 1.0)
            loss_4 = losses.mean()

            if idxbatch == 0:
                print(loss_1, loss_2, loss_3, loss_4)
            if idxbatch == -1 and epoch % 25 == 0:
                loss_scalers = np.array(
                    [loss_1.detach().cpu().numpy(),
                     loss_2.detach().cpu().numpy(),
                     loss_3.detach().cpu().numpy()])
                if np.min(loss_scalers) < 0:
                    if loss_2 < 0:
                        loss_2 = loss_2 * -1
                    else:
                        raise ValueError("One of the losses are negative")
                    print(loss_1)
                    print(loss_2)
                    print(loss_3)
                loss_scalers = loss_scalers / np.min(loss_scalers)
            loss = (loss_1 / torch.tensor(loss_scalers[0])) + (
                loss_2 / torch.tensor(loss_scalers[1])) + (
                loss_3 / torch.tensor(loss_scalers[2])) + (
                loss_4 / torch.tensor(loss_scalers[3]))
            if idxbatch == 0:
                print(loss)
            if torch.isnan(loss):
                print("Losses: {} {} {}".format(loss_1, loss_2, loss_3, loss_4))
                raise ValueError("NA occured in loss")
            # print(loss)
            loss.backward()
            optimizer.step()
            running_loss += loss
            del anchor, pos, neg, outdict_anchor, outdict_pos, outdict_neg
            # del one_hot_temp
            torch.cuda.empty_cache()
        cur_loss = running_loss / TOTBATCHIDX
        print("Epoch {}, Loss {} ({} {} {} {}) at {}".format(
            epoch, cur_loss.item(), loss_1, loss_2, loss_3, loss_4, datetime.now()))

        with open(logpath, "a+") as loglink:
            adlist = [str(epoch), str(cur_loss.item()), str(loss_1), str(loss_2), str(loss_3), str(loss_4),
                      str(idxbatch), str(datetime.now())]
            loglink.write("\t".join(adlist) + "\n")
        if epoch % 50 == 0:
            checkpoint = {
                'model': vae.state_dict(),
                'optimizer': optimizer.state_dict()
            }
            for eachpath in [modelpath, chkpath]:
                torch.save(checkpoint, eachpath)
    return vae


## this model was run separately as a separate notebook in the background which ended at this cell  
## To reload it, the data structure skeleton of the model has to be rebuilt by running the MAXEPOCH at ~ 1 or 2
## in order for you to be able to load the model trained for 400 epochs
vae = train_model(
                vae, optimizer, triplet_loader, MAXEPOCH,
                adata[randomized_cbc_list,:].X.toarray(), '/home/ssobti/projects/mir_tud/output_data/mave_output/Classification_model_R10000_K1_C1_T1/logs',
                '/home/ssobti/projects/mir_tud/output_data/mave_output/Classification_model_R10000_K1_C1_T1/models/model.pt', '/home/ssobti/projects/mir_tud/output_data/mave_output/Classification_model_R10000_K1_C1_T1/ckpt/ckpt.pt',
                loss_scalers, predict_celltypes)


### save R_tbl
R_tbl.to_csv('/home/ssobti/projects/mir_tud/output_data/mave_output/Classification_model_R10000_K1_C1_T1/R_tbl.csv')


### load in objects from MAVE_trial_pt1 second attempt
#vae.load_state_dict(torch.load('/home/ssobti/projects/mir_tud/output_data/mave_output/MAVE_trial_R10000_K1_C1_T10000/models/model.pt')['model'])


vae


adata.var.shape


vae.z_encoder.encoder.connections.sum(axis=1)


vae.z_encoder.encoder.fc_layers.Layer_0[0].bias


def apply_model(vae, expar, numlvs, MINIBATCH):
    tf_activation = None
    weight_arr = torch.mul(
         vae.z_encoder.encoder.fc_layers.Layer_0[0].weights,
         vae.z_encoder.encoder.connections).detach().cpu().numpy()
    bias_arr = vae.z_encoder.encoder.fc_layers.Layer_0[0].bias.detach().cpu().numpy()
    reconst = np.zeros(expar.shape)
    mumat = np.zeros((expar.shape[0], numlvs))
    logvarmat = np.zeros((expar.shape[0], numlvs))
    TOTBATCHIDX = int(expar.shape[0] / MINIBATCH)
    for idxbatch in range(TOTBATCHIDX):
        idxbatch_st = idxbatch * MINIBATCH
        idxbatch_end = (idxbatch + 1) * MINIBATCH
        train1 = torch.from_numpy(
            expar[idxbatch_st:idxbatch_end, :]).to(device).float()
        outdict = vae(train1)
        reconst[idxbatch_st:idxbatch_end, :] = \
            outdict["px_scale"].cpu().detach().numpy()
        mumat[idxbatch_st:idxbatch_end, :] = \
            outdict["qz_m"].cpu().detach().numpy()
        logvarmat[idxbatch_st:idxbatch_end, :] = \
            outdict["qz_v"].cpu().detach().numpy()
        if idxbatch % 100 == 0:
            print("Applied on {}/{}".format(idxbatch, TOTBATCHIDX))
    # Multiply the expar with weight_arr
    tf_activation = np.matmul(
         expar, np.transpose(weight_arr))
    tf_activation = tf_activation + bias_arr
    return reconst, mumat, logvarmat, tf_activation


get_ipython().getoutput("head /home/ssobti/projects/mir_tud/output_data/mave_output/MAVE_trial/logs/training.log.2022-11-12_12057027.684701")


get_ipython().getoutput("tail -n 25 /home/ssobti/projects/mir_tud/output_data/mave_output/MAVE_trial/logs/training.log.2022-11-12_12057027.684701")


reconst, mumat, logvarmat, rbp_act = apply_model(
                vae, adata[randomized_cbc_list,:].X.toarray(), numlvs, MINIBATCH)


reconst


reconst.shape


adata.X.toarray().shape


from scipy.stats import pearsonr
Rs = []
arrayed_data = adata[randomized_cbc_list,:].X.toarray()
for i in range(len(reconst)):
    Rs.append(pearsonr(arrayed_data[i,:], reconst[i,:])[0])


pearsonr(adata[randomized_cbc_list,:].X.toarray()[i,:], reconst[i,:])


import umap
import seaborn as sns
from matplotlib import pyplot as plt
sns.distplot(Rs)
plt.savefig("/home/ssobti/projects/mir_tud/output_data/mave_output/Classification_model_R10000_K1_C1_T1/recons_Rs.pdf")


from scipy.stats import spearmanr
Rhos = []
for i in range(len(reconst)):
    Rhos.append(spearmanr(arrayed_data[i,:], reconst[i,:])[0])


sns.distplot(Rhos)


mudf = pd.DataFrame(mumat)
mudf.columns = ["LV.mu.{}".format(each)
                            for each in range(numlvs)]

mudf.index = np.array(adata[randomized_cbc_list,:].obs.index, dtype="|U64")
mudf.head()


mudf.to_csv("/home/ssobti/projects/mir_tud/output_data/mave_output/Classification_model_R10000_K1_C1_T1/scMAVE_mu-matrix.tsv.gz",
                compression="gzip", sep="\t")


mumat = np.array(mudf.iloc[:, :numlvs])
reducer = umap.UMAP(n_neighbors=10, min_dist=0.45)
embedding = reducer.fit_transform(mumat)
umap_output = pd.DataFrame(embedding)
umap_output.columns = ["UMAP1", "UMAP2"]
umap_output["guide"] = list(adata[randomized_cbc_list,:].obs["guide"])
umap_output.index = mudf.index
umap_output.head()


umap_output.to_csv("/home/ssobti/projects/mir_tud/output_data/mave_output/Classification_model_R10000_K1_C1_T1/UMAP-OF-MU.tsv.gz",sep="\t", compression="gzip")


import pandas as pd
import seaborn as sns
import numpy as np

umap_output = pd.read_csv("/home/ssobti/projects/mir_tud/output_data/mave_output/Classification_model_R10000_K1_C1_T1/UMAP-OF-MU.tsv.gz", sep = '\t')
umap_output


umap_output['status'] = np.where(umap_output.guide=="TuD_NC", 1, 0)
sns_plot = sns.relplot(
    x="UMAP1", y="UMAP2", hue="status", data=umap_output,
    height=6, aspect=1.5)


import shap
from torch import nn

class VaePredictor(nn.Module):
    def __init__(self, net):
        '''
        Assumes 0:idx_onc are oncRNA matrix
        Assumes idx_onc:idx_sm are smRNA matrix
        assumes idx_sm: are batch
        '''
        super().__init__()
        self.net = net
        self.softmax = nn.Softmax(dim=1)
    def forward(self, x):
        # this assumes x has multiple types of data in it
        # and it's manually subsetting it here
        # you can remove reference to idx_onnc and idx_sm
        # if that's not the case
        outdict = self.net(x)
        ct_pred = self.softmax(outdict["ctpred"])
        return ct_pred
        


import random
random.seed(55)
barcodes_picked = random.sample(randomized_cbc_list, 1000)
dict_curr = {'genes': adata.var_names, 'expar': adata[barcodes_picked,:].X.toarray(), 'cell_guide': adata.obs.loc[barcodes_picked,]['guide']}


get_ipython().run_cell_magic("capture", "", """dict_out = {}
feature_names = adata.var_names
expar_all = adata[barcodes_picked,:].X.toarray()
cell_guide = adata.obs.loc[barcodes_picked,]['guide']
tensor_expar_all = torch.from_numpy(expar_all).to(device).float()
# max in memory; 128
modelForShap = VaePredictor(vae)
shapExplainer = shap.DeepExplainer(modelForShap, tensor_expar_all)
shap_values = shapExplainer.shap_values(tensor_expar_all)
dict_out["Shap.values"] = shap_values
dict_out["Input"] = expar_all
dict_out["feature_names"] = feature_names
dict_out["barcodes_chosen"] = barcodes_picked
dict_out["sample_names"] = cell_guide
dict_out["shap_sums"] = np.sum(np.abs(shap_values[1]), axis=0) 
dict_out["summary_plot1"] = shap.summary_plot(shap_values, tensor_expar_all.detach().cpu().numpy(), feature_names, max_display=300)
feature_order = np.argsort(np.sum(np.abs(shap_values[1]), axis=0))
top_features = np.array(feature_names)[feature_order[::-1][:10]]
dict_out["top_10"] = top_features
dict_out["expected_values"] = shapExplainer.expected_value
del shapExplainer
torch.cuda.empty_cache()""")


shap.summary_plot(shap_values, tensor_expar_all.detach().cpu().numpy(), feature_names, max_display=300)


test_cell_data = torch.from_numpy(adata[test_cells,:].X.toarray()).to('cpu').float()


from torch import nn
maxer = nn.Softmax(dim=1)

max_positions = []
preds = maxer(vae(test_cell_data)['ctpred'])
for idx, pred in enumerate(preds):
    max_positions.append(int(np.where(pred.cpu().detach().numpy() == max(pred).cpu().detach().numpy())[0]))



test_predictions = [one_hot_df.columns.tolist()[cell] for cell in max_positions]


actual_test_guides = list(adata[test_cells,:].obs["guide"])


len([i for i, j in zip(test_predictions, actual_test_guides) if i == j])/len(test_predictions)


filenm = '/home/ssobti/projects/mir_tud/output_data/mave_output/Classification_model_R10000_K1_C1_T1/shap_output_and_selected_cells.out'
my_shelf = shelve.open(filenm,'n') # 'n' for new
vec = ['randomized_cbc_list', 'test_cells', 'test_predictions', 'actual_test_guides', 'dict_out'] ## put objects you want to save in quotes separated by commas
for key in vec:
    try:
        my_shelf[key] = globals()[key]
    except TypeError:
        #
        # __builtins__, my_shelf, and imported modules can not be shelved.
        #
        print('ERROR shelving: {0}'.format(key))
    except pickle.PicklingError:
        print('ERROR shelving: {0}'.format(key))
    except KeyError:
        print('ERROR shelving: {0}'.format(key))
my_shelf.close()


merged = mudf.merge(adata[randomized_cbc_list,:].obs[["guide"]], left_index=True, right_index=True).groupby(["guide"]).median()
merged.head()


merged.loc["TuD_NC", ]


#set negative as origin
merged = merged - merged.loc["TuD_NC", ]
merged.drop("TuD_NC", axis=0, inplace=True)


from scipy.spatial.distance import pdist
from scipy.spatial.distance import squareform

sns.clustermap(pd.DataFrame(squareform(pdist(merged)), index=merged.index, columns=merged.index), vmax=1, figsize=(15,15))
plt.savefig("/home/ssobti/projects/mir_tud/output_data/mave_output/Classification_model_R10000_K1_C1_T1/mudf_pairwise_distance.pdf")


pd.DataFrame(squareform(pdist(merged)), index=merged.index, columns=merged.index).to_csv("/home/ssobti/projects/mir_tud/output_data/mave_output/MAVE_trial_R10000_K1_C1_T10000/mudf_pairwise_distance.tsv", sep="\t")


get_ipython().getoutput("head /home/ssobti/projects/mir_tud/output_data/mave_output/MAVE_trial_R10000_K1_C1_T10000/mudf_pairwise_distance.tsv")


from matplotlib import pyplot as plt
import seaborn as sns
sns.set(font_scale=0.5)
sns.clustermap(merged.T.corr(), figsize=(15,15))
plt.savefig("/home/ssobti/projects/mir_tud/output_data/mave_output/Classification_model_R10000_K1_C1_T1/mudf_corr.pdf")


gmtmat_df.shape


rbpdf = pd.DataFrame(rbp_act, columns=gmtmat_df.columns)
rbpdf.index = np.array(adata[randomized_cbc_list,:].obs.index, dtype="|U64")
rbpdf.head()


merged = rbpdf.merge(adata[randomized_cbc_list,:].obs[["guide"]], left_index=True, right_index=True).groupby(["guide"]).median()
merged.head()


from matplotlib import pyplot as plt
import seaborn as sns
sns.set(font_scale=0.5)
sns.clustermap(merged.T.corr(), figsize=(15,15))
plt.savefig("/home/ssobti/projects/mir_tud/output_data/mave_output/Classification_model_R10000_K1_C1_T1/rbpdf_corr.pdf")









